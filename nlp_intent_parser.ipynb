{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f0ac3",
   "metadata": {},
   "source": [
    "# **NLP Intent Parser for Industrial Technician Queries**\n",
    "\n",
    "A modular pipeline consisting of:\n",
    "1. Topic Router (LDA, SVM, Mini-BERT)\n",
    "2. Intent + Target + Parameter Token Classifier (DistilBERT, BiLSTM, LSTM)\n",
    "3. Context Resolver for domain-aware refinement\n",
    "\n",
    "This notebook demonstrates preprocessing, embeddings, token labeling, \n",
    "three different modeling strategies, evaluation, and comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebb244",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ff95169",
   "metadata": {},
   "source": [
    "### **1. Import and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f25699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (25.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cab1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: transformers in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.41.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk torch seaborn matplotlib transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5a60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lwand\\OneDrive\\Documents\\Projects\\intent_parser_nlp\\tfenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lwand\\OneDrive\\Documents\\Projects\\intent_parser_nlp\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17da02",
   "metadata": {},
   "source": [
    "###  **2. Load Technician Query Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aca78",
   "metadata": {},
   "source": [
    "**Why We Generated the Dataset Ourselves**\n",
    "\n",
    "There isn’t any publicly available dataset that captures \"technician-style\" micro-grid instructions with the level of structure we need (intent, target, parameter, modifier, conditions). Real industrial datasets are either private, messy, and rarely come with clean labels or ones we can make sense of. Since our goal here is to benchmark different NLP models, not to clean handwritten maintenance logs, synthetic data gives us full control over the balance, coverage, and consistency.\n",
    "\n",
    "It lets us shape the exact problem in the manner that we want to model, and it’s standard practice during early prototyping before fine-tuning on real operational data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8622cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/solar_ds.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f8410",
   "metadata": {},
   "source": [
    "### **3. Data Exploration (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0211f",
   "metadata": {},
   "source": [
    "**The first step is to confirm formatting and make sure all columns loaded correctly.**\n",
    "\n",
    "*Our EDA focuses on validating distribution, coverage, and linguistic variety across intents, targets, and parameters. Since the dataset is synthetic, the goal isn’t noise inspection but ensuring balance, realism, and sufficient diversity to train and compare NLP models reliably.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b25eeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>target</th>\n",
       "      <th>parameter</th>\n",
       "      <th>modifier</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Log irradiance readings on the inverter.</td>\n",
       "      <td>log</td>\n",
       "      <td>inverter</td>\n",
       "      <td>irradiance</td>\n",
       "      <td>overload</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monitor microgrid_controller — temperature see...</td>\n",
       "      <td>monitor</td>\n",
       "      <td>microgrid_controller</td>\n",
       "      <td>temperature</td>\n",
       "      <td>sudden_drop</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inspect inverter — efficiency seems critical.</td>\n",
       "      <td>inspect</td>\n",
       "      <td>inverter</td>\n",
       "      <td>efficiency</td>\n",
       "      <td>critical</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimize anomaly in inverter temperature.</td>\n",
       "      <td>optimize</td>\n",
       "      <td>inverter</td>\n",
       "      <td>temperature</td>\n",
       "      <td>high</td>\n",
       "      <td>at_night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reset anomaly in battery_bank temperature.</td>\n",
       "      <td>reset</td>\n",
       "      <td>battery_bank</td>\n",
       "      <td>temperature</td>\n",
       "      <td>high</td>\n",
       "      <td>under_cloud_cover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query    intent  \\\n",
       "0           Log irradiance readings on the inverter.       log   \n",
       "1  Monitor microgrid_controller — temperature see...   monitor   \n",
       "2      Inspect inverter — efficiency seems critical.   inspect   \n",
       "3          Optimize anomaly in inverter temperature.  optimize   \n",
       "4         Reset anomaly in battery_bank temperature.     reset   \n",
       "\n",
       "                 target    parameter     modifier         conditions  \n",
       "0              inverter   irradiance     overload  during_peak_hours  \n",
       "1  microgrid_controller  temperature  sudden_drop  during_peak_hours  \n",
       "2              inverter   efficiency     critical  during_peak_hours  \n",
       "3              inverter  temperature         high           at_night  \n",
       "4          battery_bank  temperature         high  under_cloud_cover  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb80607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>target</th>\n",
       "      <th>parameter</th>\n",
       "      <th>modifier</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>Inspect why the pv_array current is low.</td>\n",
       "      <td>inspect</td>\n",
       "      <td>pv_array</td>\n",
       "      <td>current</td>\n",
       "      <td>low</td>\n",
       "      <td>heatwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Monitor state_of_charge readings on the grid_t...</td>\n",
       "      <td>monitor</td>\n",
       "      <td>grid_tie_inverter</td>\n",
       "      <td>state_of_charge</td>\n",
       "      <td>unstable</td>\n",
       "      <td>post_storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Optimize smart_meter — current seems none.</td>\n",
       "      <td>optimize</td>\n",
       "      <td>smart_meter</td>\n",
       "      <td>current</td>\n",
       "      <td>none</td>\n",
       "      <td>under_cloud_cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Log the solar_panel fault_code.</td>\n",
       "      <td>log</td>\n",
       "      <td>solar_panel</td>\n",
       "      <td>fault_code</td>\n",
       "      <td>unstable</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Optimize issue detected in battery_bank voltage.</td>\n",
       "      <td>optimize</td>\n",
       "      <td>battery_bank</td>\n",
       "      <td>voltage</td>\n",
       "      <td>overload</td>\n",
       "      <td>at_night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query    intent  \\\n",
       "4802           Inspect why the pv_array current is low.   inspect   \n",
       "42    Monitor state_of_charge readings on the grid_t...   monitor   \n",
       "279          Optimize smart_meter — current seems none.  optimize   \n",
       "1603                    Log the solar_panel fault_code.       log   \n",
       "840    Optimize issue detected in battery_bank voltage.  optimize   \n",
       "\n",
       "                 target        parameter  modifier         conditions  \n",
       "4802           pv_array          current       low           heatwave  \n",
       "42    grid_tie_inverter  state_of_charge  unstable         post_storm  \n",
       "279         smart_meter          current      none  under_cloud_cover  \n",
       "1603        solar_panel       fault_code  unstable               none  \n",
       "840        battery_bank          voltage  overload           at_night  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d36f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   query       5000 non-null   object\n",
      " 1   intent      5000 non-null   object\n",
      " 2   target      5000 non-null   object\n",
      " 3   parameter   5000 non-null   object\n",
      " 4   modifier    5000 non-null   object\n",
      " 5   conditions  5000 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6f617",
   "metadata": {},
   "source": [
    "### **4. Preprocessing Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd2afb",
   "metadata": {},
   "source": [
    "*Even though the dataset is synthetic and noise-free, preprocessing is still required to prepare the data for deep learning models. This includes tokenization, padding/truncation to a fixed sequence length, and label encoding. We skip stopword removal, lemmatization, and other cleaning steps because our goal is to preserve the natural language variation that helps the model learn intent patterns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835fb02",
   "metadata": {},
   "source": [
    "#### **4.1 Label Encoding**\n",
    "\n",
    "We encode each structured field: intent, target, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d76989",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_encoder = LabelEncoder()\n",
    "target_encoder = LabelEncoder()\n",
    "parameter_encoder = LabelEncoder()\n",
    "\n",
    "df[\"intent_id\"] = intent_encoder.fit_transform(df[\"intent\"])\n",
    "df[\"target_id\"] = target_encoder.fit_transform(df[\"target\"])\n",
    "df[\"parameter_id\"] = parameter_encoder.fit_transform(df[\"parameter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470dafa9",
   "metadata": {},
   "source": [
    "##### **4.2 Train/Val/Test Split**\n",
    "We split once, and reuse the same split for all models to keep comparisons fair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82452662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, random_state=42, stratify=df[\"intent\"])\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.15, random_state=42, stratify=train_df[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb58b91",
   "metadata": {},
   "source": [
    "#### **4.3 Preprocessing for LSTM & Bi-LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227d55c",
   "metadata": {},
   "source": [
    "a) Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0825b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 8000  # can adjust after EDA\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecc2f3",
   "metadata": {},
   "source": [
    "b) Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f061fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(train_df[\"query\"])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df[\"query\"])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d922712",
   "metadata": {},
   "source": [
    "c) Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7803bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "X_val = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ac454",
   "metadata": {},
   "source": [
    "d) Extract Label IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341786c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_intent = train_df[\"intent_id\"].values\n",
    "y_val_intent = val_df[\"intent_id\"].values\n",
    "y_test_intent = test_df[\"intent_id\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ecf64",
   "metadata": {},
   "source": [
    "#### **4.4 Preprocessing for BERT**\n",
    "\n",
    "We will load the Tokeniser and Tokenise with Masks and Segment IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1eb77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lwand\\OneDrive\\Documents\\Projects\\intent_parser_nlp\\tfenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lwand\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=32):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for t in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            t,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    return (\n",
    "        tf.concat(input_ids, axis=0),\n",
    "        tf.concat(attention_masks, axis=0),\n",
    "    )\n",
    "\n",
    "X_train_bert_ids, X_train_bert_mask = bert_encode(train_df[\"query\"], bert_tokenizer)\n",
    "X_val_bert_ids,   X_val_bert_mask   = bert_encode(val_df[\"query\"], bert_tokenizer)\n",
    "X_test_bert_ids,  X_test_bert_mask  = bert_encode(test_df[\"query\"], bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbc9b3",
   "metadata": {},
   "source": [
    "#### **Final Note:**\n",
    "\n",
    "The preprocessing steps here ensure compatibility with both classical sequence models (LSTM/BiLSTM) and transformer-based models (BERT). Since our dataset is synthetic, the focus is not on cleaning but on formatting: tokenisation, padding, and label encoding. \n",
    "\n",
    "These steps allow us to directly compare model performance on a consistent, well-structured task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c645117",
   "metadata": {},
   "source": [
    "### **5. Topic Modeling Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b56000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b6ae",
   "metadata": {},
   "source": [
    "#### **5.1 TF-IDF + SVM Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9b70b",
   "metadata": {},
   "source": [
    "#### **5.2 LDA Topic Modeling (Unsupervised)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685da66f",
   "metadata": {},
   "source": [
    "#### **5.3 MiniBERT Topic Classifier (Supervised)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20fc4",
   "metadata": {},
   "source": [
    "### **6. Compare Topic Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa51c5",
   "metadata": {},
   "source": [
    "### **7. Token Classification Dataset Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50215bc8",
   "metadata": {},
   "source": [
    "### **8. Model 1: DistilBERT Token Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c68fcc",
   "metadata": {},
   "source": [
    "### **9. Model 2: BiLSTM Token Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923a84d",
   "metadata": {},
   "source": [
    "### **10. Model 3: Simple LSTM Tagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59fc44",
   "metadata": {},
   "source": [
    "### **11. Training Loops (All Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f2fc",
   "metadata": {},
   "source": [
    "### **12. Evaluation: Intent, Target, Parameter Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d7c06",
   "metadata": {},
   "source": [
    "### **13. Context Resolver Logic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732d98c",
   "metadata": {},
   "source": [
    "### **14. End-to-End Pipeline Demonstration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d1b32",
   "metadata": {},
   "source": [
    "### **15. Model Comparison Summary (The MLE Signal)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bb909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d80a43",
   "metadata": {},
   "source": [
    "### **16. Conclusions & Future Work**\n",
    "\n",
    "Include:\n",
    "\n",
    "- integrate with GridGuard\n",
    "\n",
    "- replace LDA with BERTopic\n",
    "\n",
    "- build your own transformer from scratch (future project)\n",
    "\n",
    "- deploy as microservice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
