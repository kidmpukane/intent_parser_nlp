{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f0ac3",
   "metadata": {},
   "source": [
    "# **NLP Intent Parser for Industrial Technician Queries**\n",
    "\n",
    "A modular pipeline consisting of:\n",
    "1. Topic Router (LDA, SVM, Mini-BERT)\n",
    "2. Intent + Target + Parameter Token Classifier (DistilBERT, BiLSTM, LSTM)\n",
    "3. Context Resolver for domain-aware refinement\n",
    "\n",
    "This notebook demonstrates preprocessing, embeddings, token labeling, \n",
    "three different modeling strategies, evaluation, and comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebb244",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ff95169",
   "metadata": {},
   "source": [
    "### **1. Import and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f25699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (25.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cab1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: transformers in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.41.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lwand\\onedrive\\documents\\projects\\intent_parser_nlp\\tfenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk torch seaborn matplotlib transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5a60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lwand\\OneDrive\\Documents\\Projects\\intent_parser_nlp\\tfenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lwand\\OneDrive\\Documents\\Projects\\intent_parser_nlp\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17da02",
   "metadata": {},
   "source": [
    "###  **2. Load Technician Query Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091aca78",
   "metadata": {},
   "source": [
    "**Why We Generated the Dataset Ourselves**\n",
    "\n",
    "There isn’t any publicly available dataset that captures \"technician-style\" micro-grid instructions with the level of structure we need (intent, target, parameter, modifier, conditions). Real industrial datasets are either private, messy, and rarely come with clean labels or ones we can make sense of. Since our goal here is to benchmark different NLP models, not to clean handwritten maintenance logs, synthetic data gives us full control over the balance, coverage, and consistency.\n",
    "\n",
    "It lets us shape the exact problem in the manner that we want to model, and it’s standard practice during early prototyping before fine-tuning on real operational data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8622cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/solar_ds.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f8410",
   "metadata": {},
   "source": [
    "### **3. Data Exploration (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0211f",
   "metadata": {},
   "source": [
    "**The first step is to confirm formatting and make sure all columns loaded correctly.**\n",
    "\n",
    "*Our EDA focuses on validating distribution, coverage, and linguistic variety across intents, targets, and parameters. Since the dataset is synthetic, the goal isn’t noise inspection but ensuring balance, realism, and sufficient diversity to train and compare NLP models reliably.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b25eeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>target</th>\n",
       "      <th>parameter</th>\n",
       "      <th>modifier</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Log irradiance readings on the inverter.</td>\n",
       "      <td>log</td>\n",
       "      <td>inverter</td>\n",
       "      <td>irradiance</td>\n",
       "      <td>overload</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monitor microgrid_controller — temperature see...</td>\n",
       "      <td>monitor</td>\n",
       "      <td>microgrid_controller</td>\n",
       "      <td>temperature</td>\n",
       "      <td>sudden_drop</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inspect inverter — efficiency seems critical.</td>\n",
       "      <td>inspect</td>\n",
       "      <td>inverter</td>\n",
       "      <td>efficiency</td>\n",
       "      <td>critical</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimize anomaly in inverter temperature.</td>\n",
       "      <td>optimize</td>\n",
       "      <td>inverter</td>\n",
       "      <td>temperature</td>\n",
       "      <td>high</td>\n",
       "      <td>at_night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reset anomaly in battery_bank temperature.</td>\n",
       "      <td>reset</td>\n",
       "      <td>battery_bank</td>\n",
       "      <td>temperature</td>\n",
       "      <td>high</td>\n",
       "      <td>under_cloud_cover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query    intent  \\\n",
       "0           Log irradiance readings on the inverter.       log   \n",
       "1  Monitor microgrid_controller — temperature see...   monitor   \n",
       "2      Inspect inverter — efficiency seems critical.   inspect   \n",
       "3          Optimize anomaly in inverter temperature.  optimize   \n",
       "4         Reset anomaly in battery_bank temperature.     reset   \n",
       "\n",
       "                 target    parameter     modifier         conditions  \n",
       "0              inverter   irradiance     overload  during_peak_hours  \n",
       "1  microgrid_controller  temperature  sudden_drop  during_peak_hours  \n",
       "2              inverter   efficiency     critical  during_peak_hours  \n",
       "3              inverter  temperature         high           at_night  \n",
       "4          battery_bank  temperature         high  under_cloud_cover  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb80607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>target</th>\n",
       "      <th>parameter</th>\n",
       "      <th>modifier</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>Monitor why the inverter efficiency is high.</td>\n",
       "      <td>monitor</td>\n",
       "      <td>inverter</td>\n",
       "      <td>efficiency</td>\n",
       "      <td>high</td>\n",
       "      <td>under_cloud_cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>Check why the battery_bank irradiance is inter...</td>\n",
       "      <td>check</td>\n",
       "      <td>battery_bank</td>\n",
       "      <td>irradiance</td>\n",
       "      <td>intermittent</td>\n",
       "      <td>during_peak_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>Reset the microgrid_controller state_of_charge.</td>\n",
       "      <td>reset</td>\n",
       "      <td>microgrid_controller</td>\n",
       "      <td>state_of_charge</td>\n",
       "      <td>sudden_drop</td>\n",
       "      <td>at_night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>Diagnose issue detected in grid_tie_inverter f...</td>\n",
       "      <td>diagnose</td>\n",
       "      <td>grid_tie_inverter</td>\n",
       "      <td>frequency</td>\n",
       "      <td>high</td>\n",
       "      <td>under_cloud_cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>Monitor the solar_panel efficiency.</td>\n",
       "      <td>monitor</td>\n",
       "      <td>solar_panel</td>\n",
       "      <td>efficiency</td>\n",
       "      <td>none</td>\n",
       "      <td>heatwave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query    intent  \\\n",
       "4591       Monitor why the inverter efficiency is high.   monitor   \n",
       "2076  Check why the battery_bank irradiance is inter...     check   \n",
       "2378    Reset the microgrid_controller state_of_charge.     reset   \n",
       "4550  Diagnose issue detected in grid_tie_inverter f...  diagnose   \n",
       "766                 Monitor the solar_panel efficiency.   monitor   \n",
       "\n",
       "                    target        parameter      modifier         conditions  \n",
       "4591              inverter       efficiency          high  under_cloud_cover  \n",
       "2076          battery_bank       irradiance  intermittent  during_peak_hours  \n",
       "2378  microgrid_controller  state_of_charge   sudden_drop           at_night  \n",
       "4550     grid_tie_inverter        frequency          high  under_cloud_cover  \n",
       "766            solar_panel       efficiency          none           heatwave  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d36f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   query       5000 non-null   object\n",
      " 1   intent      5000 non-null   object\n",
      " 2   target      5000 non-null   object\n",
      " 3   parameter   5000 non-null   object\n",
      " 4   modifier    5000 non-null   object\n",
      " 5   conditions  5000 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6f617",
   "metadata": {},
   "source": [
    "### **4. Preprocessing Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd2afb",
   "metadata": {},
   "source": [
    "*Even though the dataset is synthetic and noise-free, preprocessing is still required to prepare the data for deep learning models. This includes tokenization, padding/truncation to a fixed sequence length, and label encoding. We skip stopword removal, lemmatization, and other cleaning steps because our goal is to preserve the natural language variation that helps the model learn intent patterns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835fb02",
   "metadata": {},
   "source": [
    "#### **4.1 Label Encoding**\n",
    "\n",
    "We encode each structured field: intent, target, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d76989",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_encoder = LabelEncoder()\n",
    "target_encoder = LabelEncoder()\n",
    "parameter_encoder = LabelEncoder()\n",
    "\n",
    "df[\"intent_id\"] = intent_encoder.fit_transform(df[\"intent\"])\n",
    "df[\"target_id\"] = target_encoder.fit_transform(df[\"target\"])\n",
    "df[\"parameter_id\"] = parameter_encoder.fit_transform(df[\"parameter\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470dafa9",
   "metadata": {},
   "source": [
    "##### **4.2 Train/Val/Test Split**\n",
    "We split once, and reuse the same split for all models to keep comparisons fair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82452662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, random_state=42, stratify=df[\"intent\"])\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.15, random_state=42, stratify=train_df[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb58b91",
   "metadata": {},
   "source": [
    "#### **4.3 Preprocessing for LSTM & Bi-LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227d55c",
   "metadata": {},
   "source": [
    "a) Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0825b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 8000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecc2f3",
   "metadata": {},
   "source": [
    "b) Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f061fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(train_df[\"query\"])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df[\"query\"])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d922712",
   "metadata": {},
   "source": [
    "c) Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7803bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "X_val = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ac454",
   "metadata": {},
   "source": [
    "d) Extract Label IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341786c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (3612, 25)\n",
      "Validation set size: (638, 25)\n",
      "Test set size: (750, 25)\n",
      "Number of intent classes: 8\n",
      "Number of target classes: 8\n",
      "Number of parameter classes: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>parameter_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.535600</td>\n",
       "      <td>3.509800</td>\n",
       "      <td>4.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.283457</td>\n",
       "      <td>2.290536</td>\n",
       "      <td>2.873987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intent_id    target_id  parameter_id\n",
       "count  5000.000000  5000.000000   5000.000000\n",
       "mean      3.535600     3.509800      4.457000\n",
       "std       2.283457     2.290536      2.873987\n",
       "min       0.000000     0.000000      0.000000\n",
       "25%       2.000000     1.000000      2.000000\n",
       "50%       4.000000     4.000000      4.000000\n",
       "75%       6.000000     5.000000      7.000000\n",
       "max       7.000000     7.000000      9.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_intent = train_df[\"intent_id\"].values\n",
    "y_val_intent = val_df[\"intent_id\"].values\n",
    "y_test_intent = test_df[\"intent_id\"].values\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Number of intent classes:\", len(intent_encoder.classes_))\n",
    "print(\"Number of target classes:\", len(target_encoder.classes_))\n",
    "print(\"Number of parameter classes:\", len(parameter_encoder.classes_))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ecf64",
   "metadata": {},
   "source": [
    "#### **4.4 Preprocessing for BERT**\n",
    "\n",
    "We will load the Tokeniser and Tokenise with Masks and Segment IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=32):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for t in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            t,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    return (\n",
    "        tf.concat(input_ids, axis=0),\n",
    "        tf.concat(attention_masks, axis=0),\n",
    "    )\n",
    "\n",
    "X_train_bert_ids, X_train_bert_mask = bert_encode(train_df[\"query\"], bert_tokenizer)\n",
    "X_val_bert_ids,   X_val_bert_mask   = bert_encode(val_df[\"query\"], bert_tokenizer)\n",
    "X_test_bert_ids,  X_test_bert_mask  = bert_encode(test_df[\"query\"], bert_tokenizer)\n",
    "\n",
    "print(\"BERT Training set size:\", X_train_bert_ids.shape)\n",
    "print(\"BERT Validation set size:\", X_val_bert_ids.shape)\n",
    "print(\"BERT Test set size:\", X_test_bert_ids.shape)\n",
    "print(\"Number of intent classes:\", len(intent_encoder.classes_))\n",
    "print(\"Number of target classes:\", len(target_encoder.classes_))\n",
    "print(\"Number of parameter classes:\", len(parameter_encoder.classes_))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbc9b3",
   "metadata": {},
   "source": [
    "#### **Final Note:**\n",
    "\n",
    "The preprocessing steps here ensure compatibility with both classical sequence models (LSTM/BiLSTM) and transformer-based models (BERT). Since our dataset is synthetic, the focus is not on cleaning but on formatting: tokenisation, padding, and label encoding. \n",
    "\n",
    "These steps allow us to directly compare model performance on a consistent, well-structured task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c645117",
   "metadata": {},
   "source": [
    "### **5. Topic Modeling Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b6ae",
   "metadata": {},
   "source": [
    "#### **5.1 TF-IDF + SVM Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013d4e0",
   "metadata": {},
   "source": [
    "This baseline gives us a simple, classical machine-learning benchmark for microgrid log classification. TF-IDF converts logs into weighted token vectors, and a linear SVM separates classes in this high-dimensional space. This model sets a reference point before moving to topic models and transformer-based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb64558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 TF-IDF + SVM Baseline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Train-test split using existing splits\n",
    "X_train_tfidf = train_df[\"query\"]\n",
    "X_test_tfidf = test_df[\"query\"]\n",
    "y_train_tfidf = train_df[\"intent_id\"]\n",
    "y_test_tfidf = test_df[\"intent_id\"]\n",
    "\n",
    "# Pipeline: TF-IDF → Linear SVM\n",
    "baseline_model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\"\n",
    "    )),\n",
    "    (\"svm\", LinearSVC())\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Predictions\n",
    "preds = baseline_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_tfidf, preds))\n",
    "print(classification_report(y_test_tfidf, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9b70b",
   "metadata": {},
   "source": [
    "#### **5.2 LDA Topic Modeling (Unsupervised)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8160b5f",
   "metadata": {},
   "source": [
    "Here we use Latent Dirichlet Allocation (LDA) to uncover latent themes inside the microgrid logs without using labels. This shows whether the logs naturally cluster into meaningful operational states or fault categories. Even if LDA isn't used downstream, it helps validate the dataset structure and gives a sanity check before moving into supervised deep models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 LDA Topic Modeling\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert text → bag-of-words counts\n",
    "count_vec = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "bow = count_vec.fit_transform(df[\"query\"])\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=12,    # matching the number of intent classes\n",
    "    random_state=42,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "\n",
    "lda.fit(bow)\n",
    "\n",
    "# Display top words per topic\n",
    "def show_topics(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[-n_top_words:]]\n",
    "        print(f\"Topic {idx}: {' | '.join(top_words)}\")\n",
    "\n",
    "show_topics(lda, count_vec.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685da66f",
   "metadata": {},
   "source": [
    "#### **5.3 MiniBERT Topic Classifier (Supervised)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2b3de",
   "metadata": {},
   "source": [
    "Now we shift from unsupervised structure discovery (LDA) into supervised semantic classification.\n",
    "MiniBERT (or DistilBERT, MiniLM, etc.) will learn:\n",
    "\n",
    "    - contextual meaning\n",
    "    - operational relationships\n",
    "    - fault semantics\n",
    "    - technician-language patterns\n",
    "\n",
    "This is the backbone model for converting technician prompts into structured intent → target → parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 MiniBERT Topic Classifier\n",
    "\n",
    "%pip install \"transformers[torch]\" \"datasets\" \"accelerate>=0.26.0\" --upgrade -q\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare dataset\n",
    "# ---------------------------\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df[\"labels\"] = encoder.fit_transform(df[\"intent\"])\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"query\", \"labels\"]])\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenizer\n",
    "# ---------------------------\n",
    "\n",
    "model_name = \"prajjwal1/bert-mini\"   # ~4M params: perfect mini model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"query\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Train/validation split\n",
    "split = tokenized_ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "# ---------------------------\n",
    "# Model\n",
    "# ---------------------------\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(encoder.classes_)\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ---------------------------\n",
    "# Training configuration\n",
    "# ---------------------------\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"minibert-intent\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20fc4",
   "metadata": {},
   "source": [
    "### **6. Compare Topic Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b88b5",
   "metadata": {},
   "source": [
    "#### 6.1 Model Comparison Overview\n",
    "\n",
    "| Model               | Type          | Strengths                                 | Limitations                                  |\n",
    "|---------------------|---------------|--------------------------------------------|-----------------------------------------------|\n",
    "| TF-IDF + SVM        | Bag-of-words  | Fast, simple, strong baseline              | No context, struggles with ambiguity          |\n",
    "| LDA                 | Unsupervised  | Reveals latent structure, interpretable     | Not a classifier, weak on short text          |\n",
    "| MiniBERT            | Transformer   | Best semantic understanding, robust         | Requires GPU, slower to train                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f84e90",
   "metadata": {},
   "source": [
    "#### 6.2 Quantitative Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_accuracy = accuracy_score(y_test_tfidf, preds)\n",
    "\n",
    "# Get MiniBERT predictions on test set\n",
    "minbert_predictions = trainer.predict(val_ds)\n",
    "minbert_preds = np.argmax(minbert_predictions.predictions, axis=1)\n",
    "minbert_accuracy = accuracy_score(val_ds[\"labels\"], minbert_preds)\n",
    "\n",
    "results = {\n",
    "    \"TF-IDF + SVM\": tfidf_accuracy,\n",
    "    \"MiniBERT\": minbert_accuracy\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc56b47",
   "metadata": {},
   "source": [
    "#### 6.3 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69645035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TF-IDF Confusion Matrix\n",
    "cm = confusion_matrix(y_test_tfidf, preds)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=intent_encoder.classes_)\n",
    "disp.plot(xticks_rotation=90)\n",
    "plt.title(\"Confusion Matrix — TF-IDF + SVM\")\n",
    "plt.show()\n",
    "\n",
    "# MiniBERT Confusion Matrix\n",
    "bert_cm = confusion_matrix(val_ds[\"labels\"], np.argmax(\n",
    "    trainer.predict(val_ds).predictions, axis=1))\n",
    "bert_disp = ConfusionMatrixDisplay(bert_cm, display_labels=intent_encoder.classes_)\n",
    "bert_disp.plot(xticks_rotation=90)\n",
    "plt.title(\"Confusion Matrix — MiniBERT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d809da2",
   "metadata": {},
   "source": [
    "#### 6.4 Qualitative Comparison\n",
    "\n",
    "To test real semantic behavior, we evaluate models on ambiguous technician prompts:\n",
    "\n",
    "**Example Prompt**  \n",
    "> \"Check if the inverter is acting weird again\"\n",
    "\n",
    "**TF-IDF + SVM Output:** `monitor`  \n",
    "- Bag-of-words focuses on \"check\" and \"inverter\"  \n",
    "- No concept of “weird again” → loses semantic nuance\n",
    "\n",
    "**LDA Output:**  \n",
    "- Mostly distributes across 3–4 topics  \n",
    "- Not directly usable as a label\n",
    "\n",
    "**MiniBERT Output:** `diagnose`  \n",
    "- Understands \"acting weird\" → implies anomaly  \n",
    "- Contextually links “again” to historical faults  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c310ed6",
   "metadata": {},
   "source": [
    "#### 6.5 Conclusion\n",
    "\n",
    "Across quantitative and qualitative comparisons:\n",
    "\n",
    "- **TF-IDF + SVM** performs well when language is simple or structured but struggles with semantic nuance.  \n",
    "- **LDA** reveals that the synthetic dataset contains clean, separable topics, validating our data generation workflow.  \n",
    "- **MiniBERT** consistently provides the highest accuracy and robustness, especially on ambiguous prompts that require contextual understanding.\n",
    "\n",
    "This confirms MiniBERT as the primary model powering the intent parser in the next stages of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa51c5",
   "metadata": {},
   "source": [
    "### **7. Token Classification Dataset Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d44bd3",
   "metadata": {},
   "source": [
    "#### 7.1 Why Token Classification?\n",
    "\n",
    "Intent classification only gives us the *global* purpose of a prompt.\n",
    "But technician instructions usually contain multiple actionable elements:\n",
    "\n",
    "- the **intent** (“diagnose”, “monitor”, “adjust”)\n",
    "- the **target component** (“inverter”, “battery pack”, “PV array”)\n",
    "- the **parameter** being referenced (“temperature”, “voltage”, “output current”)\n",
    "\n",
    "A token-level BIO labeling scheme allows the model to tag each word\n",
    "with its semantic role, enabling structured extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433ae5",
   "metadata": {},
   "source": [
    "#### 7.2 BIO Label Schema\n",
    "\n",
    "We will use a minimal, highly practical schema:\n",
    "\n",
    "- `B-INTENT` — beginning token for the intent phrase\n",
    "- `I-INTENT` — continuation of the intent phrase\n",
    "- `B-TARGET` — beginning of the component being referenced\n",
    "- `I-TARGET` — continuation\n",
    "- `B-PARAM` — beginning of the parameter\n",
    "- `I-PARAM` — continuation\n",
    "- `O` — all tokens not part of our fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ddfbd",
   "metadata": {},
   "source": [
    "#### 7.3 Generate Token-Labeled Dataset from Existing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Generate BIO-labeled token dataset\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "\n",
    "def label_tokens(row):\n",
    "    text = row[\"query\"]\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    intent_words = row[\"intent\"].split()\n",
    "    target_words = row[\"target\"].split()\n",
    "    param_words = row[\"parameter\"].split()\n",
    "\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    def tag_phrase(words, tag_prefix):\n",
    "        for i in range(len(tokens)):\n",
    "            # match phrase starting at token i\n",
    "            if tokens[i:i+len(words)] == words:\n",
    "                labels[i] = f\"B-{tag_prefix}\"\n",
    "                for j in range(1, len(words)):\n",
    "                    labels[i+j] = f\"I-{tag_prefix}\"\n",
    "\n",
    "    tag_phrase(intent_words, \"INTENT\")\n",
    "    tag_phrase(target_words, \"TARGET\")\n",
    "    tag_phrase(param_words, \"PARAM\")\n",
    "\n",
    "    return pd.Series({\"tokens\": tokens, \"labels\": labels})\n",
    "\n",
    "\n",
    "bio_df = df.apply(label_tokens, axis=1)\n",
    "bio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50215bc8",
   "metadata": {},
   "source": [
    "### **8. Model 1: DistilBERT Token Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689ac96",
   "metadata": {},
   "source": [
    "#### 8.1 Notebook Markdown\n",
    "\n",
    "We now train a supervised transformer model for token-level extraction of:\n",
    "- intent\n",
    "- target component\n",
    "- parameter\n",
    "\n",
    "DistilBERT is lightweight, fast to fine-tune, and strong enough for structured text extraction. Using our BIO-tagged dataset, the model learns to highlight the exact span of each field inside a technician prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate -q\n",
    "%pip install seqeval -q\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-INTENT\",\n",
    "    2: \"I-INTENT\",\n",
    "    3: \"B-TARGET\",\n",
    "    4: \"I-TARGET\",\n",
    "    5: \"B-PARAM\",\n",
    "    6: \"I-PARAM\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Convert bio_df to Hugging Face Dataset\n",
    "tc_dataset = Dataset.from_pandas(bio_df)\n",
    "\n",
    "# Define tokenize_and_align function\n",
    "def tokenize_and_align(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tc_encoded = tc_dataset.map(tokenize_and_align, batched=True)\n",
    "tc_encoded = tc_encoded.shuffle(seed=42)\n",
    "train_test = tc_encoded.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_token_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        cur_pred = []\n",
    "        cur_lab = []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                cur_pred.append(id2label[p_i])\n",
    "                cur_lab.append(id2label[l_i])\n",
    "        true_preds.append(cur_pred)\n",
    "        true_labels.append(cur_lab)\n",
    "\n",
    "    results = metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./distilbert_token_classifier_final\")\n",
    "tokenizer.save_pretrained(\"./distilbert_token_classifier_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c68fcc",
   "metadata": {},
   "source": [
    "### **9. Model 2: BiLSTM Token Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  **9.2 Imports**\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#  **9.3 Dataset Preparation**\n",
    "\n",
    "MAX_LEN = 64\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items() if key != \"labels\"}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "#  **9.4 BiLSTM Model**\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "#  **9.5 Hyperparameters & Instantiation**\n",
    "\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size  # reuse BERT tokenizer vocab\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LABELS = len(label2id)\n",
    "\n",
    "model = BiLSTMTagger(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LABELS)\n",
    "\n",
    "#  **9.6 DataLoader**\n",
    "\n",
    "\n",
    "train_dataset = TokenDataset(encodings=train_dataset[\"input_ids\"], labels=train_dataset[\"labels\"])\n",
    "eval_dataset  = TokenDataset(encodings=eval_dataset[\"input_ids\"], labels=eval_dataset[\"labels\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader  = DataLoader(eval_dataset, batch_size=16)\n",
    "\n",
    "#  **9.7 Loss & Optimizer**\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "#  **9.8 Forward Pass (Training Loop Placeholder)**\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids)\n",
    "    loss = criterion(outputs.view(-1, NUM_LABELS), labels.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923a84d",
   "metadata": {},
   "source": [
    "### **10. Model 3: Simple LSTM Tagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10d6a3",
   "metadata": {},
   "source": [
    "#### 10. Simple LSTM Tagger\n",
    "\n",
    "This model is a straightforward, unidirectional LSTM for token-level extraction:\n",
    "- Embedding layer converts tokens to vectors\n",
    "- LSTM layer reads sequence left-to-right\n",
    "- Linear layer outputs BIO labels per token\n",
    "\n",
    "Purpose:\n",
    "- Provide a lightweight baseline\n",
    "- Compare with BiLSTM and DistilBERT\n",
    "- Highlight benefits of bidirectionality and transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff79685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
    "        super(SimpleLSTMTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
    "                            batch_first=True)  # unidirectional\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LABELS = len(label2id)\n",
    "\n",
    "simple_lstm_model = SimpleLSTMTagger(\n",
    "    VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LABELS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(simple_lstm_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa9a34",
   "metadata": {},
   "source": [
    "Simple LSTM:\n",
    "- Minimal sequential model\n",
    "- Good baseline for token classification\n",
    "- Helps quantify benefits of bidirectionality (BiLSTM) and transformers (DistilBERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59fc44",
   "metadata": {},
   "source": [
    "### **11. Training Loops (All Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f2fc",
   "metadata": {},
   "source": [
    "### **12. Evaluation: Intent, Target, Parameter Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d7c06",
   "metadata": {},
   "source": [
    "### **13. Context Resolver Logic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732d98c",
   "metadata": {},
   "source": [
    "### **14. End-to-End Pipeline Demonstration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d1b32",
   "metadata": {},
   "source": [
    "### **15. Model Comparison Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bb909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d80a43",
   "metadata": {},
   "source": [
    "### **16. Conclusions & Future Work**\n",
    "\n",
    "Include:\n",
    "\n",
    "- integrate with GridGuard\n",
    "\n",
    "- replace LDA with BERTopic\n",
    "\n",
    "- build your own transformer from scratch (future project)\n",
    "\n",
    "- deploy as microservice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
